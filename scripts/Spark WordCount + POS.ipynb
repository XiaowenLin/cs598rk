{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext, HiveContext, StorageLevel\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing other Libraries\n",
    "from pyspark.mllib.linalg import Matrices, DenseMatrix,SparseMatrix\n",
    "from np_extractor import *\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "#from rake import *\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_rdd(base, input, num_part):\n",
    "    base_dir = os.path.join(base)\n",
    "    input_path = os.path.join(input)\n",
    "    file_name = os.path.join(base_dir, input_path)\n",
    "    # load data\n",
    "    rdd = sc.textFile(file_name, num_part)\n",
    "    rdd_j = rdd.map(json.loads)\n",
    "    rdd_j.cache()\n",
    "    return rdd_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_stopwords():\n",
    "    text_file = open(\"../data/MergedStopList.txt\", \"r\")\n",
    "    lines = text_file.readlines()\n",
    "    stopwords = [\"\"]\n",
    "    for line in lines:\n",
    "        if \"#\" not in line: #Throwing out the comments\n",
    "            stopwords.append(line.strip())\n",
    "    \n",
    "    nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(nltk_stopwords)\n",
    "    return set(stopwords)\n",
    "\n",
    "stopwords = load_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanReview(text):\n",
    "    #Input a single reivew\n",
    "    text = text.split(\" \")\n",
    "    clean_text = []\n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        words = nltk.word_tokenize(word)\n",
    "        for word in words:\n",
    "            if word not in stopwords:\n",
    "                if word.isalnum():\n",
    "                    clean_text.append(word)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanReviews(reviewList):\n",
    "    #Input list of reviews\n",
    "    clean_reviews = []\n",
    "    for review in reviewList:\n",
    "        clean_reviews.extend(cleanReview(review))\n",
    "    return \" \".join(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, brand: string, categories: array<array<string>>, description: string, imUrl: string, price: double, related: struct<also_bought:array<string>,also_viewed:array<string>,bought_together:array<string>,buy_after_viewing:array<string>>, salesRank: struct<Arts, Crafts & Sewing:bigint,Automotive:bigint,Baby:bigint,Beauty:bigint,Camera &amp; Photo:bigint,Cell Phones & Accessories:bigint,Clothing:bigint,Computers & Accessories:bigint,Electronics:bigint,Grocery & Gourmet Food:bigint,Health & Personal Care:bigint,Home &amp; Kitchen:bigint,Home Improvement:bigint,Industrial & Scientific:bigint,Jewelry:bigint,Kitchen & Dining:bigint,Magazines:bigint,Movies & TV:bigint,Musical Instruments:bigint,Office Products:bigint,Patio, Lawn & Garden:bigint,Pet Supplies:bigint,Shoes:bigint,Software:bigint,Sports &amp; Outdoors:bigint,Toys & Games:bigint,Video Games:bigint,Watches:bigint>, title: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read Data file in sparkSQL\n",
    "#reviews = sqlContext.read.json(\"../data/reviews_electronics5000.json\")\n",
    "#reviews.persist(storageLevel=StorageLevel.MEMORY_AND_DISK_SER)\n",
    "revDB = sqlContext.read.json(\"../data/reviews_electronics.json\")\n",
    "metadataDB = sqlContext.read.json(\"../data/meta_electronics.json\")\n",
    "metadataDB.persist(storageLevel=StorageLevel.MEMORY_AND_DISK_SER)\n",
    "#\n",
    "# fullData = revDB.join(metadataDB)\n",
    "# fullData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('listOfSubcats','r')\n",
    "listOfSubCats = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for category in listOfSubCats:\n",
    "    selected_asin_category = metadataDB.map(lambda x: (x.asin, x.categories[0])).flatMap(lambda (asin, cats): [(asin, cat) for cat in cats]).filter(lambda (asin,cat): category in cat).map(lambda (asin,cats): (asin)).distinct()\n",
    "    \n",
    "    category_reviews = selected_asin_category.map(lambda asin: (asin,0)).join(revDB.map(lambda x: (x.asin,x.reviewText))).map(lambda joined: joined[1][1]).collect()\n",
    "    \n",
    "    outfile = open(category+'1612','w')\n",
    "    print >> outfile, \"\\n\".join(str(i) for i in category_reviews)\n",
    "    outfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading RDD and getting the data\n",
    "num_part = 16\n",
    "revs = get_rdd('../data', 'reviews_electronics5000.json', num_part)\n",
    "rev_texts = revs.map(lambda x: (x['asin'], x['reviewText']))\n",
    "just_revs = revs.map(lambda x: (x['reviewText']))\n",
    "just_revs = just_revs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For Generating Vocabulary\n",
    "#Word count & Vocabulary Building\n",
    "combined_revs = rev_texts.map(lambda (asin,text): text)\n",
    "counts = combined_revs.flatMap(lambda line: line.split(\" \"))\n",
    "counts = counts.flatMap(lambda word: nltk.word_tokenize(word))\n",
    "counts = counts.map(lambda word: (word.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
    "counts = counts.filter(lambda x: len(x[0]) > 2)\n",
    "counts = counts.filter(lambda x: x[0].isalnum())\n",
    "filteredCounts = counts.filter(lambda x: x[0] not in stopwords)\n",
    "#filteredCounts.sortBy(lambda (word, count): count)\n",
    "#countsDF = filteredCounts.toDF()\n",
    "\n",
    "vocabulary = filteredCounts.map(lambda x : x[0]).collect() #Vocab length of 13121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredCounts.toDF().sort(desc(\"_2\")).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For creating BoW model\n",
    "rev_agg_by_asin = rev_texts.map(lambda (asin, text): (asin, [text])).reduceByKey(lambda x, y: x + y)\n",
    "clean_agg_revs = rev_agg_by_asin.map(lambda x: (x[0],cleanReviews(x[1])))\n",
    "clean_agg_revs.cache()\n",
    "#stopwords = sc.broadcast(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initializing \n",
    "hashingTF = HashingTF()\n",
    "\n",
    "from collections import defaultdict\n",
    "hashMap1 = {}\n",
    "hashMap2 = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    hashMap1[hashingTF.indexOf(word)] = word\n",
    "    sparseVec = hashingTF.transform([word])\n",
    "    hashMap2[sparseVec.indices[0]] = word\n",
    "    \n",
    "featurized_reviews = clean_agg_revs.map(lambda x: (x[0], hashingTF.transform(x[1])))\n",
    "featurized_vocab = hashingTF.transform(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LDA Part\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors,DenseVector,SparseVector\n",
    "numTopics = 5\n",
    "corpus = featurized_reviews.zipWithIndex().map(lambda x: [x[1], x[0][1]]).cache()\n",
    "ldaModel = LDA.train(corpus, numTopics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabSize = ldaModel.vocabSize()\n",
    "topics = ldaModel.topicsMatrix()\n",
    "print (\"Topics shape \", topics.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tempMatrix = DenseMatrix(vocabSize,numTopics,tranTopics.flatten())\n",
    "# sparseMatrix = tempMatrix.toSparse()\n",
    "# print (sparseMatrix)\n",
    "print hashMap1[237727]\n",
    "print hashMap2[237727]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test = niceSparse(sparseMatrix)\n",
    "for topic_index in range(topics.shape[1]):\n",
    "    print(\"Words in topic %d are \\n\",topic_index)\n",
    "    for word_index in range(topics.shape[0]):\n",
    "        if topics[word_index][topic_index] != 0:\n",
    "            print word_index in hashMap2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SparseMatrix to co-ordinate tuples\n",
    "def niceSparse(self):    \n",
    "        niceSparseMat = []\n",
    "        cur_col = 0\n",
    "        smlist = []\n",
    "\n",
    "        zipindval = zip(self.rowIndices, self.values)\n",
    "        for i, (rowInd, value) in enumerate(zipindval):\n",
    "            if self.colPtrs[cur_col + 1] <= i:\n",
    "                cur_col += 1\n",
    "            if self.isTransposed:\n",
    "                niceSparseMat.append((cur_col,rowInd,value))\n",
    "            else:\n",
    "                niceSparseMat.append((rowInd,cur_col,value))\n",
    "        return niceSparseMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#POS Tagging these items\n",
    "# posTaggedWords = filteredCounts.map(lambda (word,count): (word,nltk.pos_tag(word)[0][1],count))\n",
    "#posTaggedWords = posTaggedWords.cache()\n",
    "#posTaggedWords.persist(storageLevel=StorageLevel.MEMORY_AND_DISK_SER)\n",
    "# df = pd.DataFrame(posTaggedWords.collect())\n",
    "# df.to_csv('../data/processed/posTaggedWords_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Syntax for NLTK\n",
    "#tokens = nltk.word_tokenize(text)\n",
    "#tagged = nltk.pos_tag(tokens)\n",
    "#from nltk.corpus import stopwords\n",
    "#stopwords.words('english')\n",
    "#nltk_stopwords = stopwords.words('english')\n",
    "#other_stopwords = \n",
    "#from nltk.corpus import wordnet as wn\n",
    "# words = data.flatMap(lambda x: nltk.word_tokenize(x))\n",
    "# print words.take(10)\n",
    "# pos_word = words.map(lambda x: nltk.pos_tag([x]))\n",
    "# print pos_word.take(5)\n",
    "\n",
    "a = featurized_reviews.collect()[1]\n",
    "print len(a[1].toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "X = lda.datasets.load_reuters()\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "print(X.shape)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#X.sum()\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. output\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(items_np.collect())\n",
    "# df.to_csv('data/processed/computers_kw.csv')\n",
    "reviews = sc.textFile('Digital SLR Cameras1612_200.txt,NAS_200.txt')\n",
    "just_clean_reviews = reviews.map(lambda x: cleanReviews([x])).collect()\n",
    "# just_clean_reviews = clean_agg_revs.map(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in just_clean_reviews:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    #stopped_tokens = [i for i in tokens if not i in stopwords]\n",
    "    stopped_tokens = [i for i in tokens]\n",
    "    \n",
    "    \n",
    "    # stem tokens\n",
    "    #stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    stemmed_tokens = [i for i in stopped_tokens]\n",
    "\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print clean_agg_revs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.013*drive + 0.012*device + 0.011*drives + 0.010*nas + 0.008*network + 0.008*files + 0.007*access + 0.007*data + 0.007*server + 0.006*storage + 0.005*raid + 0.005*unit + 0.005*backup + 0.005*media + 0.005*home + 0.005*time + 0.005*2 + 0.005*usb + 0.005*web + 0.004*hard + 0.004*box + 0.004*wd + 0.004*sharespace + 0.004*setup + 0.004*file'),\n",
       " (1,\n",
       "  u'0.034*camera + 0.009*lens + 0.008*nikon + 0.007*lenses + 0.006*quality + 0.005*pentax + 0.005*time + 0.005*pictures + 0.005*digital + 0.004*canon + 0.004*price + 0.004*cameras + 0.004*love + 0.004*data + 0.004*focus + 0.003*easy + 0.003*battery + 0.003*buy + 0.003*d2x + 0.003*lot + 0.003*shoot + 0.003*unit + 0.003*dslr + 0.003*body + 0.003*product')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=2, num_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
